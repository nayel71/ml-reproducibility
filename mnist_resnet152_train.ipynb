{"cells":[{"cell_type":"markdown","metadata":{"id":"UEBilEjLj5wY"},"source":["Deep Learning Models -- A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.\n","- Author: Sebastian Raschka\n","- GitHub Repository: https://github.com/rasbt/deeplearning-models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOzuY8Yvj5wb"},"outputs":[],"source":["#%load_ext watermark\n","#%watermark -a 'Sebastian Raschka' -v -p torch"]},{"cell_type":"markdown","metadata":{"id":"rH4XmErYj5wm"},"source":["# Model Zoo -- CNN Gender Classifier (ResNet-50 Architecture, CelebA) with Data Parallelism"]},{"cell_type":"markdown","metadata":{"id":"klxXyY4BGD43"},"source":["### Network Architecture"]},{"cell_type":"markdown","metadata":{"id":"lJAlnyHMGD43"},"source":["The network in this notebook is an implementation of the ResNet-50 [1] architecture on the CelebA face dataset [2] to train a gender classifier.  \n","\n","\n","References\n","    \n","- [1] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). ([CVPR Link](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html))\n","\n","- [2] Zhang, K., Tan, L., Li, Z., & Qiao, Y. (2016). Gender and smile classification using deep convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 34-38).\n","\n","The ResNet-50 architecture is similar to the ResNet-34 architecture shown below (from [1]):\n","\n","\n","![](../images/resnets/resnet34/resnet34-arch.png)\n","\n","However, in ResNet-50, the skip connection uses a bottleneck (from [1]):\n","\n","\n","![](../images/resnets/resnet50/resnet50-arch-1.png)\n"]},{"cell_type":"markdown","metadata":{"id":"esrchyZtGD44"},"source":["The following figure illustrates residual blocks with skip connections such that the input passed via the shortcut matches the dimensions of the main path's output, which allows the network to learn identity functions.\n","\n","![](../images/resnets/resnet-ex-1-1.png)\n","\n","\n","The ResNet-34 architecture actually uses residual blocks with skip connections such that the input passed via the shortcut matches is resized to dimensions of the main path's output. Such a residual block is illustrated below:\n","\n","![](../images/resnets/resnet-ex-1-2.png)\n","\n","The ResNet-50 uses a bottleneck as shown below:\n","\n","![](../images/resnets/resnet-ex-1-3.png)"]},{"cell_type":"markdown","metadata":{"id":"Saw_6f9RGD44"},"source":["For a more detailed explanation see the other notebook, [resnet-ex-1.ipynb](resnet-ex-1.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"MkoGLH_Tj5wn"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORj09gnrj5wp"},"outputs":[],"source":["import os\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","\n","if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"I6hghKPxj5w0"},"source":["## Model Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnT0sZIwj5wu"},"outputs":[],"source":["##########################\n","### SETTINGS\n","##########################\n","\n","# Hyperparameters\n","RANDOM_SEED = 1\n","LEARNING_RATE = 0.0001\n","BATCH_SIZE = 128\n","NUM_EPOCHS = 20\n","\n","# Architecture\n","NUM_FEATURES = 28*28\n","NUM_CLASSES = 10\n","\n","# Other\n","DEVICE = \"cuda:0\"\n","GRAYSCALE = True"]},{"cell_type":"markdown","metadata":{"id":"q6Ag1IqCGD45"},"source":["### MNIST Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHDuYCYAGD46","executionInfo":{"status":"ok","timestamp":1668499452878,"user_tz":300,"elapsed":7,"user":{"displayName":"Samin Riasat","userId":"08227986502864479244"}},"outputId":"090192f5-0485-4171-b391-6a16c0171541"},"outputs":[{"output_type":"stream","name":"stdout","text":["Image batch dimensions: torch.Size([128, 1, 28, 28])\n","Image label dimensions: torch.Size([128])\n"]}],"source":["##########################\n","### MNIST DATASET\n","##########################\n","\n","# Note transforms.ToTensor() scales input images\n","# to 0-1 range\n","\n","train_dataset = MNIST(root='data', \n","                      train=True, \n","                      transform=transforms.ToTensor(),\n","                      download=True)\n","\n","test_dataset = MNIST(root='data', \n","                     train=False, \n","                     transform=transforms.ToTensor())\n","\n","\n","train_loader = DataLoader(dataset=train_dataset, \n","                          batch_size=BATCH_SIZE, \n","                          shuffle=True)\n","\n","test_loader = DataLoader(dataset=test_dataset, \n","                         batch_size=BATCH_SIZE, \n","                         shuffle=False)\n","\n","# Checking the dataset\n","for images, labels in train_loader:  \n","    print('Image batch dimensions:', images.shape)\n","    print('Image label dimensions:', labels.shape)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LEezQpHOGD46","executionInfo":{"status":"ok","timestamp":1668499457391,"user_tz":300,"elapsed":4519,"user":{"displayName":"Samin Riasat","userId":"08227986502864479244"}},"outputId":"bcb52f98-2b82-4c55-f666-b68019dc3200"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Batch index: 0 | Batch size: 128\n","Epoch: 2 | Batch index: 0 | Batch size: 128\n"]}],"source":["device = torch.device(DEVICE)\n","torch.manual_seed(0)\n","\n","for epoch in range(2):\n","    for batch_idx, (x, y) in enumerate(train_loader):\n","        print('Epoch:', epoch+1, end='')\n","        print(' | Batch index:', batch_idx, end='')\n","        print(' | Batch size:', y.size()[0])\n","        x = x.to(device)\n","        y = y.to(device)\n","        break"]},{"cell_type":"markdown","metadata":{"id":"T0J0qsa5GD46"},"source":["The following code cell that implements the ResNet-34 architecture is a derivative of the code provided at https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85bBrFLOGD47"},"outputs":[],"source":["##########################\n","### MODEL\n","##########################\n","\n","\n","\"\"\"\n","ResNet adapted from:\n","    https://github.com/kefth/fashion-mnist\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"3x3 convolution with padding\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block=BasicBlock, layers=[2, 2, 2, 2], num_classes=10, size_for_cifar=True):\n","        self.inplanes = 64\n","        super(ResNet, self).__init__()\n","        self.size_for_cifar = size_for_cifar\n","        if size_for_cifar:\n","            num_channels = 3\n","        else:\n","            num_channels = 1\n","\n","        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=3, stride=1, padding=1,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","        self.avgpool = nn.AvgPool2d(4)\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, (2. / n)**.5)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # because MNIST is already 1x1 here:\n","        # disable avg pooling\n","        if self.size_for_cifar:\n","            x = self.avgpool(x)\n","\n","        x = x.view(x.size(0), -1)\n","        logits = self.fc(x)\n","        probas = F.softmax(logits, dim=1)\n","        return logits, probas \n","\n","\n","class ResNet18(ResNet):\n","    def __init__(self, num_classes=10, size_for_cifar=True):\n","        super(ResNet18, self).__init__(\n","                block=BasicBlock, layers=[2, 2, 2, 2],\n","                num_classes=num_classes, size_for_cifar=size_for_cifar)\n","\n","\n","class ResNet152(ResNet):\n","    def __init__(self, num_classes=100, size_for_cifar=True):\n","        super(ResNet152, self).__init__(\n","                block=BasicBlock, layers=[3, 8, 36, 3],\n","                num_classes=num_classes, size_for_cifar=size_for_cifar)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lza9t_uj5w1"},"outputs":[],"source":["torch.manual_seed(RANDOM_SEED)\n","\n","model = ResNet152(size_for_cifar=False)\n","model.to(DEVICE)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "]},{"cell_type":"markdown","metadata":{"id":"RAodboScj5w6"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":732},"executionInfo":{"elapsed":3541,"status":"error","timestamp":1668499463884,"user":{"displayName":"Samin Riasat","userId":"08227986502864479244"},"user_tz":300},"id":"Dzh3ROmRj5w7","outputId":"6476454b-a063-4da2-b3fa-82150b27cba8","scrolled":false},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-86ce96dcd765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m### FORWARD AND BACK PROP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-7036da3fab1a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x8192 and 512x100)"]}],"source":["def compute_accuracy(model, data_loader, device):\n","    correct_pred, num_examples = 0, 0\n","    for i, (features, targets) in enumerate(data_loader):\n","        features = features.to(device)\n","        targets = targets.to(device)\n","        logits, probas = model(features)\n","        _, predicted_labels = torch.max(probas, 1)\n","        num_examples += targets.size(0)\n","        correct_pred += (predicted_labels == targets).sum()\n","    return correct_pred.float()/num_examples * 100\n","    \n","\n","start_time = time.time()\n","for epoch in range(NUM_EPOCHS):    \n","    model.train()\n","    for batch_idx, (features, targets) in enumerate(train_loader):\n","        features = features.to(DEVICE)\n","        targets = targets.to(DEVICE)\n","        ### FORWARD AND BACK PROP\n","        logits, probas = model(features)\n","        cost = F.cross_entropy(logits, targets)\n","        optimizer.zero_grad()\n","        cost.backward()\n","        ### UPDATE MODEL PARAMETERS\n","        optimizer.step()\n","        ### LOGGING\n","        if not batch_idx % 50:\n","            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n","                   %(epoch+1, NUM_EPOCHS, batch_idx, \n","                     len(train_loader), cost))\n","    model.eval()\n","    with torch.set_grad_enabled(False): # save memory during inference\n","        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n","              epoch+1, NUM_EPOCHS, \n","              compute_accuracy(model, train_loader, device=DEVICE)))\n","    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n","print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"]},{"cell_type":"markdown","metadata":{"id":"paaeEQHQj5xC"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzQMWKq5j5xE"},"outputs":[],"source":["with torch.set_grad_enabled(False): # save memory during inference\n","    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cgz5TcVsGD49"},"outputs":[],"source":["for batch_idx, (features, targets) in enumerate(test_loader):\n","    features = features\n","    targets = targets\n","    break\n","    \n","\n","nhwc_img = np.transpose(features[0], axes=(1, 2, 0))\n","nhw_img = np.squeeze(nhwc_img.numpy(), axis=2)\n","plt.imshow(nhw_img, cmap='Greys');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gjmzxoxlGD49"},"outputs":[],"source":["model.eval()\n","logits, probas = model(features.to(device)[0, None])\n","print('Probability 7 %.2f%%' % (probas[0][7]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cp4aapOuGD49"},"outputs":[],"source":["#%watermark -iv"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"27C476-eGQKl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PATH = \"/content/drive/My Drive/Colab Notebooks/EECS553ML_reproduce/\"\n","torch.save(model.state_dict(), f\"{PATH}mnist_resnet152_model.t7\")"],"metadata":{"id":"8odwXS21LnHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NpV3CgKQyu8x"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"371px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":0}